# -*- coding: utf-8 -*-
"""A1_exampleCommented.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18f1jwj36AIAADSo2eWKz37Oj8UZI3IB_
"""

# download data (-q is the quiet mode)
! wget -q https://www.dropbox.com/s/lhb1awpi769bfdr/test.csv?dl=1 -O test.csv # download test data from dropbox and store in file
! wget -q https://www.dropbox.com/s/gudb5eunj700s7j/train.csv?dl=1 -O train.csv # download train data from dropbox and store in file

import pandas as pd                                     # import library for data processing, CSV file input/output and data manipulation

Xy_train = pd.read_csv('train.csv', engine='python')    # read training data from file and store in variable

X_train = Xy_train.drop(columns=['price_rating'])       # store training data without the output in variable
y_train = Xy_train[['price_rating']]                    # store training data output in variable

print('traning', len(X_train))                          # print length (how many columns) of training data
Xy_train.price_rating.hist()                            # Make a histogram of the tran data frame’s based on the price rating to get a better understanding how the data looks like

X_test = pd.read_csv('test.csv', engine='python')       # read training data from file and store in variable
testing_ids = X_test.Id                                 # store the IDs of testing data in variable
print('testing', len(X_test))                           # print length (how many columns) of testing data

# model training and tuning
import numpy as np  # library for linear algebra
from sklearn.compose import ColumnTransformer # to transform columns seperately
from sklearn.datasets import fetch_openml # Fetch dataset from openml by name or dataset id.
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer # for completing missing values
from sklearn.preprocessing import StandardScaler, OneHotEncoder # standarsize data by removing the mean and scaling to unit variance, encode categorial features
from sklearn.linear_model import LogisticRegression # to apply the logistic regression
from sklearn.model_selection import train_test_split, GridSearchCV  # to split the data into test and train
from xgboost.sklearn import XGBClassifier #to apply XGBoost

np.random.seed(0) # generate random seed for generating consintent testing

numeric_features = ['bedrooms', 'review_scores_location', 'accommodates', 'beds'] # select the numeric features
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])  #Sequentially apply a list of transforms and a final estimator; SimpleImputer: replaces missing value with median for numeric_features
                                    #StandardScaler: Normalize the data 

categorical_features = [
  'property_type', 'is_business_travel_ready', 'room_type', ] # select the categorical features 
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])  #OneHotEncoder: transform into vector

 #   Applies transformers to columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)]) # transforms data

 #create Pipeline that applies preprocessor which was defined above and then uses xgb as classifier
 
regr = Pipeline(steps=[('preprocessor', preprocessor),
                      ('regressor', XGBClassifier(
                          objective='multi:softmax', seed=1))])


#set training and testing data with the chosen numerical and categorical features; so that the others are not noted anymore
X_train = X_train[[*numeric_features, *categorical_features]] 
X_test = X_test[[*numeric_features, *categorical_features]]

# `__` denotes attribute 
# (e.g. regressor__n_estimators means the `n_estimators` param for `regressor`
#  which is our xgb)
param_grid = {
    'preprocessor__num__imputer__strategy': ['mean'],
    'regressor__n_estimators': [50, 100], # test first 50 and then 100 estimators
    'regressor__max_depth':[10, 20]     # use a maximum test depth of 10 and 20
}


# GridSearchCV: implements a “fit” and a “score” method. Find the best parameters to use.
#   Do Cross-validation 5 times
#   n_jobs = 2 means running two jobs in parallel
#   verbose - give messages
#   evaluate the test set on accuracy
grid_search = GridSearchCV(
    regr, param_grid, cv=5, verbose=3, n_jobs=2, 
    scoring='accuracy')

grid_search.fit(X_train, y_train) #fitting model with data
print('best score {}'.format(grid_search.best_score_))  #printing the accuracy

# Prediction & generating the submission file
y_pred = grid_search.predict(X_test)
pd.DataFrame(
    {'Id': testing_ids, 'price_rating':y_pred}).to_csv('sample_submission.csv', index=False)