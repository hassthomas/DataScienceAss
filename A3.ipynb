{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fQhSr_LqSIg4","colab_type":"code","colab":{}},"source":["!wget -q https://l1nna.com/372/Assignment/A2-3/train.csv\n","!wget -q https://l1nna.com/372/Assignment/A2-3/test.csv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uaABvQpluwjs","colab_type":"code","outputId":"05f1fa9c-ca53-4e38-b58b-35ca76b8210a","executionInfo":{"status":"ok","timestamp":1586176613595,"user_tz":-120,"elapsed":848,"user":{"displayName":"Thomas Hass","photoUrl":"","userId":"07524976909550567906"}},"colab":{"base_uri":"https://localhost:8080/","height":435}},"source":["# import libraries\n","import pandas as pd\n","import csv\n","\n","# read train data from file and save in variable\n","xy_train_df = pd.read_csv('train.csv')\n","\n","# read test data from file and save in variable; use id as row label\n","x_test_df  = pd.read_csv('test.csv', index_col='id')\n","\n","# calculate length of review description (how many characters) and sort the table by the length(ascending)\n","xy_train_df['length'] = xy_train_df.apply(lambda x: len(x.review), axis=1)\n","xy_train_df = xy_train_df.sort_values('length')\n","xy_train_df"],"execution_count":8,"outputs":[{"output_type":"stream","text":["6223\n","2667\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>rating</th>\n","      <th>review</th>\n","      <th>length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6037</th>\n","      <td>2596</td>\n","      <td>1</td>\n","      <td>Five Stars_GOOD</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>5353</th>\n","      <td>4643</td>\n","      <td>1</td>\n","      <td>Love it_Love it</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>2545</th>\n","      <td>8791</td>\n","      <td>1</td>\n","      <td>Five Stars_Good</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>3902</th>\n","      <td>6098</td>\n","      <td>1</td>\n","      <td>Five Stars_love!</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>2850</th>\n","      <td>4609</td>\n","      <td>1</td>\n","      <td>love these_so cute!</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5651</th>\n","      <td>518</td>\n","      <td>1</td>\n","      <td>So far, it's awesome_Ok, so I'll say up front ...</td>\n","      <td>5765</td>\n","    </tr>\n","    <tr>\n","      <th>1615</th>\n","      <td>124</td>\n","      <td>1</td>\n","      <td>It Works (Read Tips For Potential Effectivenes...</td>\n","      <td>6740</td>\n","    </tr>\n","    <tr>\n","      <th>5046</th>\n","      <td>7257</td>\n","      <td>1</td>\n","      <td>An exquisitely effective product with an astou...</td>\n","      <td>8082</td>\n","    </tr>\n","    <tr>\n","      <th>4859</th>\n","      <td>7555</td>\n","      <td>1</td>\n","      <td>Gorgeous professional looking manicure at home...</td>\n","      <td>8134</td>\n","    </tr>\n","    <tr>\n","      <th>2758</th>\n","      <td>4823</td>\n","      <td>1</td>\n","      <td>WAITED TO WRITE THIS REVIEW UNTIL AFTER READIN...</td>\n","      <td>12773</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6223 rows Ã— 4 columns</p>\n","</div>"],"text/plain":["        id  rating                                             review  length\n","6037  2596       1                                    Five Stars_GOOD      15\n","5353  4643       1                                    Love it_Love it      15\n","2545  8791       1                                    Five Stars_Good      15\n","3902  6098       1                                   Five Stars_love!      16\n","2850  4609       1                                love these_so cute!      19\n","...    ...     ...                                                ...     ...\n","5651   518       1  So far, it's awesome_Ok, so I'll say up front ...    5765\n","1615   124       1  It Works (Read Tips For Potential Effectivenes...    6740\n","5046  7257       1  An exquisitely effective product with an astou...    8082\n","4859  7555       1  Gorgeous professional looking manicure at home...    8134\n","2758  4823       1  WAITED TO WRITE THIS REVIEW UNTIL AFTER READIN...   12773\n","\n","[6223 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"dHl0DGCyvA7l","colab_type":"code","outputId":"6af25646-1e76-4533-ee4e-c0e34f889142","executionInfo":{"status":"ok","timestamp":1586177783210,"user_tz":-120,"elapsed":2090,"user":{"displayName":"Thomas Hass","photoUrl":"","userId":"07524976909550567906"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["#import libraries\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from sklearn.model_selection import train_test_split\n","\n","#define vocabulary size and max_len\n","vocab_size = 10000\n","max_len = 256\n","\n","#split training set into training and validation set; 80% training, 20% validation\n","xy_train, xy_validation = train_test_split(xy_train_df, test_size=0.2)\n","\n","# print length to be sure about split\n","print(len(xy_train))\n","print(len(xy_validation))\n","\n","# build vocabulary from training set\n","tokenizer = Tokenizer(num_words=vocab_size)\n","tokenizer.fit_on_texts(xy_train.review)\n","\n","# padding is done inside; convert review text to numpy matrix for max length 256 for training, validation and testing set\n","x_train = tokenizer.texts_to_matrix(xy_train.review, mode='binary')[:, :max_len]\n","y_train = xy_train.rating\n","\n","x_valid = tokenizer.texts_to_matrix(xy_validation.review, mode='binary')[:, :max_len]\n","y_valid = xy_validation.rating\n","\n","x_test = tokenizer.texts_to_matrix(x_test_df.review, mode='binary')[:, :max_len]\n","\n","print(x_train.shape)\n","print(x_valid.shape)\n","print(x_test.shape)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["4978\n","1245\n","<keras_preprocessing.text.Tokenizer object at 0x7f8245549940>\n","(4978, 256)\n","(1245, 256)\n","(2667, 256)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_EtEv2RivFKP","colab_type":"code","outputId":"3d777da3-b603-4c65-f0fa-09b7e0becf6e","executionInfo":{"status":"ok","timestamp":1586177497599,"user_tz":-120,"elapsed":63064,"user":{"displayName":"Thomas Hass","photoUrl":"","userId":"07524976909550567906"}},"colab":{"base_uri":"https://localhost:8080/","height":521}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import collections\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","\n","#building model using keras sequential \n","model = keras.Sequential()\n","# Add an Embedding layer expecting input vocab of size vocab_size, and\n","# output embedding dimension of size 20\n","model.add(keras.layers.Embedding(vocab_size, 20))\n","# Add a LSTM layer with 128 internal units\n","model.add(keras.layers.LSTM(100))\n","# Add a Dense layer with 1 unit\n","model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n","\n","# compile the model with \"binary_crossentropy\" as loss function\n","# set the metrics to accuracy\n","# Use the Adam optimizer\n","model.compile(\n","    optimizer=Adam(clipnorm=4.),\n","    loss='binary_crossentropy',\n","    metrics=['accuracy'])\n","\n","# train network in 15 epochs with batch_size of 64\n","history = model.fit(x_train,\n","                    y_train,\n","                    epochs=15,\n","                    batch_size=64,\n","                    validation_data=(x_valid, y_valid),\n","                    verbose=1)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Epoch 1/15\n","78/78 [==============================] - 4s 57ms/step - loss: 0.4019 - accuracy: 0.8803 - val_loss: 0.4051 - val_accuracy: 0.8594\n","Epoch 2/15\n","78/78 [==============================] - 4s 46ms/step - loss: 0.3675 - accuracy: 0.8803 - val_loss: 0.4086 - val_accuracy: 0.8594\n","Epoch 3/15\n","78/78 [==============================] - 4s 46ms/step - loss: 0.3673 - accuracy: 0.8803 - val_loss: 0.4029 - val_accuracy: 0.8594\n","Epoch 4/15\n","78/78 [==============================] - 4s 46ms/step - loss: 0.3681 - accuracy: 0.8803 - val_loss: 0.4049 - val_accuracy: 0.8594\n","Epoch 5/15\n","78/78 [==============================] - 4s 46ms/step - loss: 0.3671 - accuracy: 0.8803 - val_loss: 0.4039 - val_accuracy: 0.8594\n","Epoch 6/15\n","78/78 [==============================] - 4s 46ms/step - loss: 0.3677 - accuracy: 0.8803 - val_loss: 0.4052 - val_accuracy: 0.8594\n","Epoch 7/15\n","78/78 [==============================] - 4s 45ms/step - loss: 0.3677 - accuracy: 0.8803 - val_loss: 0.4028 - val_accuracy: 0.8594\n","Epoch 8/15\n","78/78 [==============================] - 4s 45ms/step - loss: 0.3674 - accuracy: 0.8803 - val_loss: 0.4046 - val_accuracy: 0.8594\n","Epoch 9/15\n","78/78 [==============================] - 4s 45ms/step - loss: 0.3675 - accuracy: 0.8803 - val_loss: 0.4052 - val_accuracy: 0.8594\n","Epoch 10/15\n","78/78 [==============================] - 4s 45ms/step - loss: 0.3663 - accuracy: 0.8803 - val_loss: 0.4042 - val_accuracy: 0.8594\n","Epoch 11/15\n","78/78 [==============================] - 3s 45ms/step - loss: 0.3661 - accuracy: 0.8803 - val_loss: 0.4026 - val_accuracy: 0.8594\n","Epoch 12/15\n","78/78 [==============================] - 4s 45ms/step - loss: 0.3661 - accuracy: 0.8803 - val_loss: 0.4023 - val_accuracy: 0.8594\n","Epoch 13/15\n","78/78 [==============================] - 3s 45ms/step - loss: 0.3665 - accuracy: 0.8803 - val_loss: 0.4041 - val_accuracy: 0.8594\n","Epoch 14/15\n","78/78 [==============================] - 4s 45ms/step - loss: 0.3669 - accuracy: 0.8803 - val_loss: 0.4036 - val_accuracy: 0.8594\n","Epoch 15/15\n","78/78 [==============================] - 3s 45ms/step - loss: 0.3655 - accuracy: 0.8803 - val_loss: 0.4073 - val_accuracy: 0.8594\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z_naN0rn3OlN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"a18be6f0-cba1-4ae7-9da0-b447376747f1","executionInfo":{"status":"ok","timestamp":1586177505513,"user_tz":-120,"elapsed":1776,"user":{"displayName":"Thomas Hass","photoUrl":"","userId":"07524976909550567906"}}},"source":["# evaluate model\n","model.evaluate(x_valid, y_valid)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["39/39 [==============================] - 1s 18ms/step - loss: 0.4110 - accuracy: 0.8594\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.4110211133956909, 0.8594377636909485]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"Byj-weFZvaaS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"b9ae4ab5-7ebb-488b-dd72-88e97f9388b4","executionInfo":{"status":"ok","timestamp":1586177509967,"user_tz":-120,"elapsed":1713,"user":{"displayName":"Thomas Hass","photoUrl":"","userId":"07524976909550567906"}}},"source":["y_predict = np.squeeze(model.predict_classes(x_valid))\n","\n","from sklearn.metrics import  f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","print(f1_score(y_valid, y_predict, average='micro'))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-13-c215bc58b971>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n","Instructions for updating:\n","Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","0.8594377510040161\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8FDSaQaxvG0q","colab_type":"code","colab":{}},"source":["# run on testing set:\n","y_predict = np.squeeze(model.predict_classes(x_test))\n","\n","pd.DataFrame(\n","    {'id': x_test_df.index,\n","     'rating':y_predict}).to_csv('sample_submission.csv', index=False)"],"execution_count":0,"outputs":[]}]}