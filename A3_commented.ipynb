{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fQhSr_LqSIg4","colab_type":"code","colab":{}},"source":["!wget -q https://l1nna.com/372/Assignment/A2-3/train.csv\n","!wget -q https://l1nna.com/372/Assignment/A2-3/test.csv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uaABvQpluwjs","colab_type":"code","outputId":"2374445b-9a70-4019-c17b-5e071a8d8e22","executionInfo":{"status":"ok","timestamp":1586547403962,"user_tz":-120,"elapsed":657,"user":{"displayName":"Thomas Hass","photoUrl":"","userId":"07524976909550567906"}},"colab":{"base_uri":"https://localhost:8080/","height":402}},"source":["# import libraries\n","import pandas as pd\n","import csv\n","\n","# read train data from file and save in variable\n","xy_train_df = pd.read_csv('train.csv')\n","\n","# read test data from file and save in variable; use id as row label\n","x_test_df  = pd.read_csv('test.csv', index_col='id')\n","\n","# calculate length of review description (how many characters) and sort the table by the length(ascending)\n","xy_train_df['length'] = xy_train_df.apply(lambda x: len(x.review), axis=1)\n","xy_train_df = xy_train_df.sort_values('length')\n","xy_train_df"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>rating</th>\n","      <th>review</th>\n","      <th>length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6037</th>\n","      <td>2596</td>\n","      <td>1</td>\n","      <td>Five Stars_GOOD</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>5353</th>\n","      <td>4643</td>\n","      <td>1</td>\n","      <td>Love it_Love it</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>2545</th>\n","      <td>8791</td>\n","      <td>1</td>\n","      <td>Five Stars_Good</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>3902</th>\n","      <td>6098</td>\n","      <td>1</td>\n","      <td>Five Stars_love!</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>2850</th>\n","      <td>4609</td>\n","      <td>1</td>\n","      <td>love these_so cute!</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5651</th>\n","      <td>518</td>\n","      <td>1</td>\n","      <td>So far, it's awesome_Ok, so I'll say up front ...</td>\n","      <td>5765</td>\n","    </tr>\n","    <tr>\n","      <th>1615</th>\n","      <td>124</td>\n","      <td>1</td>\n","      <td>It Works (Read Tips For Potential Effectivenes...</td>\n","      <td>6740</td>\n","    </tr>\n","    <tr>\n","      <th>5046</th>\n","      <td>7257</td>\n","      <td>1</td>\n","      <td>An exquisitely effective product with an astou...</td>\n","      <td>8082</td>\n","    </tr>\n","    <tr>\n","      <th>4859</th>\n","      <td>7555</td>\n","      <td>1</td>\n","      <td>Gorgeous professional looking manicure at home...</td>\n","      <td>8134</td>\n","    </tr>\n","    <tr>\n","      <th>2758</th>\n","      <td>4823</td>\n","      <td>1</td>\n","      <td>WAITED TO WRITE THIS REVIEW UNTIL AFTER READIN...</td>\n","      <td>12773</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6223 rows Ã— 4 columns</p>\n","</div>"],"text/plain":["        id  rating                                             review  length\n","6037  2596       1                                    Five Stars_GOOD      15\n","5353  4643       1                                    Love it_Love it      15\n","2545  8791       1                                    Five Stars_Good      15\n","3902  6098       1                                   Five Stars_love!      16\n","2850  4609       1                                love these_so cute!      19\n","...    ...     ...                                                ...     ...\n","5651   518       1  So far, it's awesome_Ok, so I'll say up front ...    5765\n","1615   124       1  It Works (Read Tips For Potential Effectivenes...    6740\n","5046  7257       1  An exquisitely effective product with an astou...    8082\n","4859  7555       1  Gorgeous professional looking manicure at home...    8134\n","2758  4823       1  WAITED TO WRITE THIS REVIEW UNTIL AFTER READIN...   12773\n","\n","[6223 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"dHl0DGCyvA7l","colab_type":"code","outputId":"0ab6a607-b16d-48ce-8fc9-8d08fabbfad9","executionInfo":{"status":"ok","timestamp":1586547425551,"user_tz":-120,"elapsed":2392,"user":{"displayName":"Thomas Hass","photoUrl":"","userId":"07524976909550567906"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["#import libraries\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from sklearn.model_selection import train_test_split\n","\n","#define vocabulary size and max_len\n","vocab_size = 10000\n","max_len = 256\n","\n","#split training set into training and validation set; 80% training, 20% validation\n","xy_train, xy_validation = train_test_split(xy_train_df, test_size=0.2)\n","\n","# print length to be sure about split\n","print(len(xy_train))\n","print(len(xy_validation))\n","\n","# build vocabulary from training set\n","tokenizer = Tokenizer(num_words=vocab_size)\n","tokenizer.fit_on_texts(xy_train.review)\n","\n","# padding is done inside; convert review text to numpy matrix for max length 256 for training, validation and testing set (so that network can deal with it)\n","x_train = tokenizer.texts_to_matrix(xy_train.review, mode='binary')[:, :max_len]\n","y_train = xy_train.rating\n","\n","x_valid = tokenizer.texts_to_matrix(xy_validation.review, mode='binary')[:, :max_len]\n","y_valid = xy_validation.rating\n","\n","x_test = tokenizer.texts_to_matrix(x_test_df.review, mode='binary')[:, :max_len]\n","\n","print(x_train.shape)\n","print(x_valid.shape)\n","print(x_test.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["4978\n","1245\n","(4978, 256)\n","(1245, 256)\n","(2667, 256)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_EtEv2RivFKP","colab_type":"code","outputId":"835ad8b8-11a8-448a-c88d-1a1b7dfc8229","executionInfo":{"status":"ok","timestamp":1586547639218,"user_tz":-120,"elapsed":61153,"user":{"displayName":"Thomas Hass","photoUrl":"","userId":"07524976909550567906"}},"colab":{"base_uri":"https://localhost:8080/","height":756}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import collections\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","\n","#building model using keras sequential \n","model = keras.Sequential()\n","# Add an Embedding layer expecting input vocab of size vocab_size, and\n","# output embedding dimension of size 20\n","model.add(keras.layers.Embedding(vocab_size, 20))\n","# Add a LSTM layer with 128 internal units\n","model.add(keras.layers.LSTM(100))\n","# Add a Dense layer with 1 unit\n","model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n","\n","model.summary()\n","\n","# compile the model with \"binary_crossentropy\" as loss function\n","# set the metrics to accuracy\n","# Use the Adam optimizer\n","model.compile(\n","    optimizer=Adam(clipnorm=4.),\n","    loss='binary_crossentropy',\n","    metrics=['accuracy'])\n","\n","# train network in 15 epochs with batch_size of 64\n","history = model.fit(x_train,\n","                    y_train,\n","                    epochs=15,\n","                    batch_size=64,\n","                    validation_data=(x_valid, y_valid),\n","                    verbose=1)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, None, 20)          200000    \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 100)               48400     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 101       \n","=================================================================\n","Total params: 248,501\n","Trainable params: 248,501\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/15\n","78/78 [==============================] - 4s 55ms/step - loss: 0.4139 - accuracy: 0.8682 - val_loss: 0.4013 - val_accuracy: 0.8627\n","Epoch 2/15\n","78/78 [==============================] - 4s 46ms/step - loss: 0.3687 - accuracy: 0.8795 - val_loss: 0.4024 - val_accuracy: 0.8627\n","Epoch 3/15\n","78/78 [==============================] - 4s 46ms/step - loss: 0.3693 - accuracy: 0.8795 - val_loss: 0.4031 - val_accuracy: 0.8627\n","Epoch 4/15\n","78/78 [==============================] - 3s 45ms/step - loss: 0.3693 - accuracy: 0.8795 - val_loss: 0.4040 - val_accuracy: 0.8627\n","Epoch 5/15\n","78/78 [==============================] - 3s 44ms/step - loss: 0.3689 - accuracy: 0.8795 - val_loss: 0.4068 - val_accuracy: 0.8627\n","Epoch 6/15\n","78/78 [==============================] - 3s 43ms/step - loss: 0.3688 - accuracy: 0.8795 - val_loss: 0.4002 - val_accuracy: 0.8627\n","Epoch 7/15\n","78/78 [==============================] - 3s 43ms/step - loss: 0.3695 - accuracy: 0.8795 - val_loss: 0.4005 - val_accuracy: 0.8627\n","Epoch 8/15\n","78/78 [==============================] - 3s 43ms/step - loss: 0.3684 - accuracy: 0.8795 - val_loss: 0.4005 - val_accuracy: 0.8627\n","Epoch 9/15\n","78/78 [==============================] - 3s 43ms/step - loss: 0.3683 - accuracy: 0.8795 - val_loss: 0.4025 - val_accuracy: 0.8627\n","Epoch 10/15\n","78/78 [==============================] - 3s 43ms/step - loss: 0.3692 - accuracy: 0.8795 - val_loss: 0.4028 - val_accuracy: 0.8627\n","Epoch 11/15\n","78/78 [==============================] - 3s 43ms/step - loss: 0.3684 - accuracy: 0.8795 - val_loss: 0.4008 - val_accuracy: 0.8627\n","Epoch 12/15\n","78/78 [==============================] - 3s 43ms/step - loss: 0.3678 - accuracy: 0.8795 - val_loss: 0.4018 - val_accuracy: 0.8627\n","Epoch 13/15\n","78/78 [==============================] - 3s 43ms/step - loss: 0.3653 - accuracy: 0.8795 - val_loss: 0.4082 - val_accuracy: 0.8627\n","Epoch 14/15\n","78/78 [==============================] - 3s 43ms/step - loss: 0.3661 - accuracy: 0.8795 - val_loss: 0.3992 - val_accuracy: 0.8627\n","Epoch 15/15\n","78/78 [==============================] - 3s 43ms/step - loss: 0.3632 - accuracy: 0.8795 - val_loss: 0.4019 - val_accuracy: 0.8627\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z_naN0rn3OlN","colab_type":"code","outputId":"a18be6f0-cba1-4ae7-9da0-b447376747f1","executionInfo":{"status":"ok","timestamp":1586177505513,"user_tz":-120,"elapsed":1776,"user":{"displayName":"Thomas Hass","photoUrl":"","userId":"07524976909550567906"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# evaluate model\n","model.evaluate(x_valid, y_valid)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["39/39 [==============================] - 1s 18ms/step - loss: 0.4110 - accuracy: 0.8594\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.4110211133956909, 0.8594377636909485]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"Byj-weFZvaaS","colab_type":"code","outputId":"b9ae4ab5-7ebb-488b-dd72-88e97f9388b4","executionInfo":{"status":"ok","timestamp":1586177509967,"user_tz":-120,"elapsed":1713,"user":{"displayName":"Thomas Hass","photoUrl":"","userId":"07524976909550567906"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["y_predict = np.squeeze(model.predict_classes(x_valid))\n","\n","from sklearn.metrics import  f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","print(f1_score(y_valid, y_predict, average='micro'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-13-c215bc58b971>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n","Instructions for updating:\n","Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","0.8594377510040161\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8FDSaQaxvG0q","colab_type":"code","colab":{}},"source":["# run on testing set:\n","y_predict = np.squeeze(model.predict_classes(x_test))\n","\n","pd.DataFrame(\n","    {'id': x_test_df.index,\n","     'rating':y_predict}).to_csv('sample_submission.csv', index=False)"],"execution_count":0,"outputs":[]}]}